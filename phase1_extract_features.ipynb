{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing audio files: 100%|██████████| 10/10 [00:01<00:00,  5.04it/s]\n",
      "Processing audio files: 100%|██████████| 10/10 [00:01<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to MongoDB\n",
      "Data inserted into MongoDB collection 'features'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Define file paths\n",
    "metadata_path = '/home/abubakar/Documents/BDA/Project/fma_metadata/tracks.csv'\n",
    "audio_dir = '/home/abubakar/Documents/BDA/Project/fma_medium'\n",
    "\n",
    "# Load tracks metadata\n",
    "tracks = pd.read_csv(metadata_path, header=[0, 1], index_col=0)\n",
    "\n",
    "# Select medium subset tracks\n",
    "medium_tracks = tracks[tracks['set', 'subset'] == 'medium']\n",
    "\n",
    "# Get audio files\n",
    "audio_files = []\n",
    "for root, dirs, files in os.walk(audio_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp3'):\n",
    "            audio_files.append(os.path.join(root, file))\n",
    "\n",
    "# Create MongoDB client and database\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['music']\n",
    "collection = db['features']\n",
    "\n",
    "# Create an empty DataFrame to store features\n",
    "features_df = pd.DataFrame(columns=[\n",
    "                           'file_path', 'track_id', 'duration', 'mfcc', 'spectral_centroid', 'zero_crossing_rate'])\n",
    "\n",
    "# Function to extract audio features\n",
    "\n",
    "\n",
    "def extract_audio_features(file_path, sr=22050):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(file_path, sr=sr)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13).flatten()\n",
    "\n",
    "        # Extract spectral centroid\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(\n",
    "            y=audio, sr=sr).flatten()\n",
    "\n",
    "        # Extract zero-crossing rate\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(\n",
    "            audio).flatten()\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = librosa.get_duration(y=audio, sr=sr)\n",
    "\n",
    "        return duration, mfcc, spectral_centroid, zero_crossing_rate\n",
    "\n",
    "    except (librosa.util.exceptions.ParameterError, Exception) as e:\n",
    "        print(f\"Error processing file {file_path}: {e}. Skipping this file.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def pad_array(arr, max_length):\n",
    "    current_length = arr.shape[0]\n",
    "    if current_length < max_length:\n",
    "        pad_width = ((0, max_length - current_length),) + ((0, 0),) * (arr.ndim - 1)\n",
    "        return np.pad(arr, pad_width, mode='constant')\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "max_lengths = {\"mfcc\": 0, \"spectral_centroid\": 0, \"zero_crossing_rate\": 0}\n",
    "\n",
    "samples = 10\n",
    "\n",
    "# Iterate over audio files with tqdm, limit to 1500 files\n",
    "for file_path in tqdm(audio_files[:samples], desc=\"Processing audio files\"):\n",
    "    track_id = int(os.path.basename(file_path)[:-4])\n",
    "    duration, mfcc_features, spectral_centroid, zero_crossing_rate = extract_audio_features(\n",
    "        file_path, sr=22050)\n",
    "    if len(mfcc_features) > max_lengths[\"mfcc\"]:\n",
    "        max_lengths[\"mfcc\"] = len(mfcc_features)\n",
    "    elif len(spectral_centroid) > max_lengths[\"spectral_centroid\"]:\n",
    "        max_lengths[\"spectral_centroid\"] = len(spectral_centroid)\n",
    "    elif len(zero_crossing_rate) > max_lengths[\"zero_crossing_rate\"]:\n",
    "        max_lengths[\"zero_crossing_rate\"] = len(zero_crossing_rate)\n",
    "\n",
    "\n",
    "for file_path in tqdm(audio_files[:samples], desc=\"Processing audio files\"):\n",
    "    track_id = int(os.path.basename(file_path)[:-4])\n",
    "    duration, mfcc_features, spectral_centroid, zero_crossing_rate = extract_audio_features(\n",
    "        file_path, sr=22050)\n",
    "    mfcc_features = pad_array(mfcc_features, max_lengths[\"mfcc\"])\n",
    "    spectral_centroid = pad_array(spectral_centroid, max_lengths[\"spectral_centroid\"])\n",
    "    zero_crossing_rate = pad_array(zero_crossing_rate, max_lengths[\"zero_crossing_rate\"])\n",
    "\n",
    "    if duration is not None and mfcc_features is not None:\n",
    "        # features_df.loc[len(features_df)] = {'file_path': file_path,\n",
    "        #                                      'track_id': track_id, 'duration': duration,\n",
    "        #                                      'mfcc': mfcc_features,\n",
    "        #                                      'spectral_centroid': spectral_centroid,\n",
    "        #                                      'zero_crossing_rate': zero_crossing_rate}\n",
    "        # Insert data into MongoDB collection\n",
    "        # record_obj = {f\"mfcc_{i}\":mfcc_features[i].item() for i in range(500)}\n",
    "        record_obj = {}\n",
    "        # record_obj[\"mfcc\"] = mfcc_features.tolist()\n",
    "        record_obj[\"file_path\"] = file_path\n",
    "        record_obj[\"track_id\"] = track_id\n",
    "        record_obj[\"duration\"] = duration\n",
    "        record_obj[\"zero_crossing_rate\"] = zero_crossing_rate.tolist()\n",
    "        # record_obj[\"spectral_centroid\"] = spectral_centroid.tolist()\n",
    "\n",
    "        collection.insert_one(record_obj)\n",
    "\n",
    "# # Round duration and convert track_id to int\n",
    "# features_df['track_id'] = features_df['track_id'].astype(int)\n",
    "# features_df['duration'] = features_df['duration'].round(2)SSSS\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "# features_df.to_csv('audio_features_1425.csv', index=False)\n",
    "# print(\"DataFrame saved to 'audio_features_1425.csv'\")\n",
    "\n",
    "# Save DataFrame to MongoDB\n",
    "# records = features_df.to_dict(orient='records')\n",
    "# collection.insert_many(records)\n",
    "print(\"Data saved to MongoDB\")\n",
    "\n",
    "\n",
    "# Close the MongoClient connection\n",
    "client.close()\n",
    "\n",
    "# Print a message indicating completion\n",
    "print(\"Data inserted into MongoDB collection 'features'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mfcc': 16809, 'spectral_centroid': 1293, 'zero_crossing_rate': 1293}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 20:49:26 WARN Utils: Your hostname, abubakar-Lenovo-Flex-7-14IAU7 resolves to a loopback address: 127.0.1.1; using 192.168.100.63 instead (on interface wlp0s20f3)\n",
      "24/05/12 20:49:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/abubakar/Downloads/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/abubakar/Downloads/spark-3.1.1-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/abubakar/.ivy2/cache\n",
      "The jars for the packages stored in: /home/abubakar/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fc9eae27-67a6-4eef-b78f-f03d7c966e36;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.3.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      ":: resolution report :: resolve 6279ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.3.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fc9eae27-67a6-4eef-b78f-f03d7c966e36\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/3ms)\n",
      "24/05/12 20:49:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/12 20:49:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Music Recommendation\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://127.0.0.1:27017/music.features\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://127.0.0.1:27017/music.features\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39125/1955035675.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_pandas['spectral_centroid'].fillna(value=pd.Series([[]]*len(df_pandas)),inplace=True),\n",
      "/tmp/ipykernel_39125/1955035675.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_pandas['zero_crossing_rate'].fillna(value=pd.Series([[]]*len(df_pandas)),inplace=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field duration: DoubleType can not accept object '/home/abubakar/Documents/BDA/Project/fma_medium/120/120147.mp3' in type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 33\u001b[0m\n\u001b[1;32m     21\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     22\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     23\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_crossing_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, ArrayType(DoubleType()), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m ])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Convert Pandas DataFrame to Spark DataFrame\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m df_spark \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pandas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/session.py:673\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    670\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/pandas/conversion.py:300\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    299\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/session.py:700\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    698\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/session.py:509\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 509\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    512\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/session.py:682\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m--> 682\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/types.py:1409\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj):\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1409\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/types.py:1390\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1386\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1387\u001b[0m             new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of object (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) does not match with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1388\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of fields (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(obj), \u001b[38;5;28mlen\u001b[39m(verifiers))))\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 1390\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1392\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/types.py:1409\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj):\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1409\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/types.py:1403\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_default\u001b[39m(obj):\n\u001b[1;32m   1402\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 1403\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Spotify/lib/python3.12/site-packages/pyspark/sql/types.py:1291\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj):\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 1291\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m can not accept object \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m in type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1292\u001b[0m                                 \u001b[38;5;241m%\u001b[39m (dataType, obj, \u001b[38;5;28mtype\u001b[39m(obj))))\n",
      "\u001b[0;31mTypeError\u001b[0m: field duration: DoubleType can not accept object '/home/abubakar/Documents/BDA/Project/fma_medium/120/120147.mp3' in type <class 'str'>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType,StructField,StringType,DoubleType,IntegerType,ArrayType\n",
    "# detch data from mongo db \n",
    "data=list(collection.find())\n",
    "# convert mongo db to pandas data frame \n",
    "df_pandas=pd.DataFrame(data)\n",
    "\n",
    "# replace non value with empty list \n",
    "df_pandas[\"mfcc\"].fillna(value=pd.Series([[]]*len(df_pandas))),\n",
    "df_pandas['spectral_centroid'].fillna(value=pd.Series([[]]*len(df_pandas)),inplace=True),\n",
    "df_pandas['zero_crossing_rate'].fillna(value=pd.Series([[]]*len(df_pandas)),inplace=True)\n",
    "\n",
    "# Convert duration field to numeric\n",
    "df_pandas['duration'] = pd.to_numeric(df_pandas['duration'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"file_path\", StringType(), True),\n",
    "    StructField(\"mfcc\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"spectral_centroid\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"track_id\", IntegerType(), True),\n",
    "    StructField(\"zero_crossing_rate\", ArrayType(DoubleType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read \\\n",
    ".format(\"mongodb\") \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "array_to_vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "df_new = df_spark.select(array_to_vector_udf(df_spark['zero_crossing_rate']).alias('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$PYSPARK_DRIVER_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: string, duration: double, file_path: string, track_id: int, zero_crossing_rate: array<double>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "(training, test) = df_spark.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------+--------------------+\n",
      "|                 _id|          duration|           file_path|track_id|  zero_crossing_rate|\n",
      "+--------------------+------------------+--------------------+--------+--------------------+\n",
      "|6640c65da1a916e6d...|29.976598639455784|/home/abubakar/Do...|  120167|[0.09716796875, 0...|\n",
      "|6640c65da1a916e6d...|29.976598639455784|/home/abubakar/Do...|  120475|[0.11328125, 0.13...|\n",
      "|6640c65da1a916e6d...|29.976598639455784|/home/abubakar/Do...|  120932|[0.01416015625, 0...|\n",
      "|6640c65ea1a916e6d...|29.988979591836735|/home/abubakar/Do...|  120334|[0.02197265625, 0...|\n",
      "|6640c65ea1a916e6d...|29.976598639455784|/home/abubakar/Do...|  120163|[0.0927734375, 0....|\n",
      "+--------------------+------------------+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf, col, lit\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "\n",
    "# Function to pad feature vectors to a fixed length\n",
    "def pad_features(feat, max_len):\n",
    "    padded_feat = feat + [0.0] * (max_len - len(feat))\n",
    "    return Vectors.dense(padded_feat)\n",
    "\n",
    "# UDF to pad feature vectors\n",
    "pad_features_udf = udf(lambda feat, max_len: pad_features(feat, max_len), VectorUDT())\n",
    "\n",
    "\n",
    "def apply_pca(df, input_col, k):\n",
    "    # Find the maximum length of feature vectors\n",
    "    max_len = df.selectExpr(\"size({}) as size\".format(input_col)).agg({\"size\": \"max\"}).collect()[0][\"max(size)\"]\n",
    "    \n",
    "    # Pad feature vectors to the maximum length\n",
    "    df = df.withColumn(\"padded_features\", pad_features_udf(col(input_col), lit(max_len)))\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(k=k, inputCol=\"padded_features\", outputCol=\"pca_features\")\n",
    "    pca_model = pca.fit(df)\n",
    "    df_pca = pca_model.transform(df)\n",
    "    \n",
    "    return df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- mfcc: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- spectral_centroid: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- track_id: integer (nullable = true)\n",
      " |-- zero_crossing_rate: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.columns\n",
    "df_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n",
      "|zero_crossing_rate| feature_vector|\n",
      "+------------------+---------------+\n",
      "|       0.072265625|  [0.072265625]|\n",
      "|     0.09130859375|[0.09130859375]|\n",
      "|       0.111328125|  [0.111328125]|\n",
      "|     0.06396484375|[0.06396484375]|\n",
      "|     0.05419921875|[0.05419921875]|\n",
      "|      0.0517578125| [0.0517578125]|\n",
      "|        0.05078125|   [0.05078125]|\n",
      "|         0.0546875|    [0.0546875]|\n",
      "|      0.0830078125| [0.0830078125]|\n",
      "|     0.15283203125|[0.15283203125]|\n",
      "|      0.1826171875| [0.1826171875]|\n",
      "|     0.24560546875|[0.24560546875]|\n",
      "|     0.25146484375|[0.25146484375]|\n",
      "|     0.18212890625|[0.18212890625]|\n",
      "|      0.1455078125| [0.1455078125]|\n",
      "|     0.07861328125|[0.07861328125]|\n",
      "|            0.0625|       [0.0625]|\n",
      "|      0.0673828125| [0.0673828125]|\n",
      "|      0.0908203125| [0.0908203125]|\n",
      "|     0.13427734375|[0.13427734375]|\n",
      "+------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "# explode the array into individual rows\n",
    "exploded_df = df_spark.select(explode(\"zero_crossing_rate\").alias(\"zero_crossing_rate\"))\n",
    "\n",
    "assembler=VectorAssembler(inputCols=[\"zero_crossing_rate\"],outputCol=\"feature_vector\")\n",
    "spark_df = assembler.transform(exploded_df)\n",
    "\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- mfcc: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- spectral_centroid: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- track_id: integer (nullable = true)\n",
      " |-- zero_crossing_rate: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(mfcc=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select(\"mfcc\").limit(1).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_vector_udf = udf(lambda arr: Vectors.dense(arr), VectorUDT())\n",
    "df_new = df_spark.select(array_to_vector_udf(df_spark['zero_crossing_rate']).alias('features'), df_spark['track_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- track_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import col,monotonically_increasing_id\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "    \n",
    "    \n",
    "\n",
    "# Assuming 'df' is your DataFrame with features, and 'features_col' is the name of the column containing features\n",
    "# 'k' is the number of nearest neighbors you want to find\n",
    "def approximate_nearest_neighbors(spark_df, features_col='zero', k=5):\n",
    "    # Create an LSH model\n",
    "\n",
    "    \n",
    "    brp_lsh = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10)\n",
    "    \n",
    "    # Fit the model\n",
    "    model = brp_lsh.fit(spark_df)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def find_similar_song(spark_df, query_id, model, k):\n",
    "    query_df = spark_df.filter(col(\"_id\") == query_id)\n",
    "    # Get the original ID\n",
    "    # Approximate nearest neighbor search\n",
    "    df_nn = model.approxNearestNeighbors(query_df, spark_df, k, distCol=\"distance\")\n",
    "\n",
    "    df_nn = df_nn.withColumn(\"track_id\", monotonically_increasing_id())\n",
    "    \n",
    "    return df_nn.select(\"track_id\", \"distance\", \"hashes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|feature_vector |\n",
      "+---------------+\n",
      "|[0.072265625]  |\n",
      "|[0.09130859375]|\n",
      "|[0.111328125]  |\n",
      "|[0.06396484375]|\n",
      "|[0.05419921875]|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the structure of the feature_vector column\n",
    "spark_df.select(\"feature_vector\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import col,monotonically_increasing_id\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "    \n",
    "    \n",
    "\n",
    "# Assuming 'df' is your DataFrame with features, and 'features_col' is the name of the column containing features\n",
    "# 'k' is the number of nearest neighbors you want to find\n",
    "def approximate_nearest_neighbors(spark_df, features_col='zero', k=5):\n",
    "    # Create an LSH model\n",
    "    assembler = VectorAssembler(inputCols=[features_col], outputCol=\"features_vector\")\n",
    "    spark_df = assembler.transform(spark_df)\n",
    "    print(spark_df)\n",
    "\n",
    "    \n",
    "    brp_lsh = BucketedRandomProjectionLSH(inputCol=\"features_vector\", outputCol=\"hashes\", numHashTables=10)\n",
    "    \n",
    "    # Fit the model\n",
    "    model = brp_lsh.fit(spark_df)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def find_similar_song(spark_df, query_id, model, k):\n",
    "    query_df = spark_df.filter(col(\"_id\") == query_id)\n",
    "    # Get the original ID\n",
    "    # Approximate nearest neighbor search\n",
    "    df_nn = model.approxNearestNeighbors(query_df, spark_df, k, distCol=\"distance\")\n",
    "\n",
    "    df_nn = df_nn.withColumn(\"track_id\", monotonically_increasing_id())\n",
    "    \n",
    "    return df_nn.select(\"track_id\", \"distance\", \"hashes\")\n",
    "\n",
    "# Example usage:\n",
    "# df_nn = approximate_nearest_neighbors(df, \"features\")\n",
    "# df_nn.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "def array_to_vector(arr):\n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "new_df = df_spark.withColumn(\"mfcc_vector\", array_to_vector_udf(col(\"mfcc\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- mfcc: vector (nullable = true)\n",
      " |-- spectral_centroid: vector (nullable = true)\n",
      " |-- track_id: integer (nullable = true)\n",
      " |-- zero_crossing_rate: vector (nullable = true)\n",
      " |-- mfcc_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- file_path: string (nullable = true)\n",
      " |-- mfcc: vector (nullable = true)\n",
      " |-- spectral_centroid: vector (nullable = true)\n",
      " |-- track_id: integer (nullable = true)\n",
      " |-- zero_crossing_rate: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "\n",
    "feature_columns = df_spark.columns[2:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(training, test) = df_spark.randomSplit([0.8, 0.2])\n",
    "# Prepare the RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"mfcc\", labelCol=\"rating\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_id: string, duration: double, file_path: string, mfcc: vector, spectral_centroid: vector, track_id: int, zero_crossing_rate: vector]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
